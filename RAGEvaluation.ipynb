{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ak265/anaconda3/envs/llm-training/lib/python3.10/site-packages/langchain/indexes/vectorstore.py:129: UserWarning: Using InMemoryVectorStore as the default vectorstore.This memory store won't persist data. You should explicitlyspecify a vectorstore when using VectorstoreIndexCreator\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.evaluation.qa import QAEvalChain\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "\n",
    "# Load the PDF document and create the vector store index\n",
    "loader = PyPDFLoader(\"policy-booklet-0923.pdf\")\n",
    "\n",
    "document = loader.load_and_split()\n",
    "# Creates a vector index from the extracted documents.\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "index = VectorstoreIndexCreator(embedding=embeddings).from_documents(document)\n",
    "\n",
    "# Initialize the RAG model and language model\n",
    "llm  = ChatOpenAI(model_name=\"gpt-3.5-turbo\",temperature=0)\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"refine\", retriever=index.vectorstore.as_retriever())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df=pd.read_excel('./EvaluationDataset.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=[]\n",
    "for i in range(len(df)):   \n",
    "    question=df.loc[i,\"Query\"]\n",
    "    answer=df.loc[i,\"Response\"]\n",
    "    dataset.append({\"query\": question,\n",
    "                    \"answer\": answer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "template = \"\"\"You are a teacher evaluating answers. \n",
    "You are given a question, my answer, and the true answer, and are asked to score  my answer as either CORRECT or INCORRECT.\n",
    "\n",
    "Example Format:\n",
    "QUESTION: question here\n",
    "MY ANSWER: my answer here\n",
    "TRUE ANSWER: true answer here\n",
    "GRADE: CORRECT or INCORRECT here\n",
    "\n",
    "Grade my answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between my answer and true answer. It is OK if my answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \n",
    "\n",
    "QUESTION: {query}\n",
    "MY ANSWER: {result}\n",
    "TRUE ANSWER: {answer}\n",
    "GRADE:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "GRADE_ANSWER_PROMPT = PromptTemplate(input_variables=[\"query\", \"result\", \"answer\"], template=template)\n",
    "\n",
    "def grade_model_answer(predicted_dataset: List, predictions: List) -> List:\n",
    "    \"\"\"\n",
    "    Grades the distilled answer based on ground truth and model predictions.\n",
    "    @param predicted_dataset: A list of dictionaries containing ground truth questions and answers.\n",
    "    @param predictions: A list of dictionaries containing model predictions for the questions.\n",
    "    @param grade_answer_prompt: The prompt level for the grading. Either \"Fast\" or \"Full\".\n",
    "    @return: A list of scores for the distilled answers.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set the grading prompt based on the grade_answer_prompt parameter\n",
    "    prompt = GRADE_ANSWER_PROMPT\n",
    "\n",
    "    # Create an evaluation chain\n",
    "    eval_chain = QAEvalChain.from_llm(\n",
    "        llm=ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0),\n",
    "        prompt=prompt\n",
    "    )\n",
    "    for pred in predictions:\n",
    "        if not pred['result']:\n",
    "            pred['result'] = 'No answer provided'\n",
    "\n",
    "    # Evaluate the predictions and ground truth using the evaluation chain\n",
    "    graded_outputs = eval_chain.evaluate(\n",
    "        predicted_dataset,\n",
    "        predictions,\n",
    "        question_key=\"query\",\n",
    "        prediction_key=\"result\"\n",
    "    )\n",
    "\n",
    "    return graded_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_dataset=[]\n",
    "for i in range(len(dataset)):\n",
    "    query=dataset[i]['query']\n",
    "    result=qa.invoke({\"query\": query})\n",
    "    prediction_dataset.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grade the model answers\n",
    "graded_outputs = grade_model_answer(dataset[:21], prediction_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 76.19 %\n"
     ]
    }
   ],
   "source": [
    "correct_count =0\n",
    "for i in range(len(graded_outputs)):\n",
    "    if graded_outputs[i]['results'].split(':')[1].strip()=='CORRECT':\n",
    "        correct_count+=1\n",
    "print(f\"Accuracy:\",round(correct_count/len(graded_outputs)*100,2),\"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
